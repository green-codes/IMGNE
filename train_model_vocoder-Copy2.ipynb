{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    \n",
    "# hack: change the default (master) device\n",
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reader = wave.open('feather.wav', 'rb')\n",
    "print(reader.getframerate(), reader.getsampwidth(), reader.getnframes())\n",
    "nframes = reader.getnframes()\n",
    "d = np.empty(nframes)\n",
    "d = np.frombuffer(reader.readframes(nframes), dtype=np.float32, count=nframes)\n",
    "\n",
    "fs, data = wavfile.read('feather.wav')\n",
    "print(data.shape, d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataloader inits\n",
    "# from util import *\n",
    "\n",
    "# chunk_size = 20\n",
    "# window_size = 2048\n",
    "# window_overlap = 1024\n",
    "# batch_size = 1\n",
    "\n",
    "# train_files = ['feather.wav'] \n",
    "# # train_files = ['data/05_8K.wav']#, 'data/02.wav', 'data/03.wav']#, 'data/04_8K.wav', 'data/05_8K.wav']\n",
    "# val_files = ['data/01_8K.wav']\n",
    "\n",
    "# train_gen = DataGenerator(train_files, chunk_size, window_size, \n",
    "#                           window_overlap, batch_size, vocoder=True)\n",
    "# val_gen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/imgne/maestro-v1.0.0/2017/MIDI-Unprocessed_066_PIANO066_MID--AUDIO-split_07-07-17_Piano-e_3-02_wav--3.wav\n"
     ]
    }
   ],
   "source": [
    "# import maestro\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "maestro_root = \"/imgne/maestro-v1.0.0/\"\n",
    "\n",
    "meta_df_orig = pd.read_csv(maestro_root + \"maestro-v1.0.0.csv\")\n",
    "meta_df_orig[\"audio_filename\"] = meta_df_orig[\"audio_filename\"].apply(lambda x: maestro_root + x)\n",
    "meta_df = meta_df_orig[meta_df_orig[\"year\"] == 2017]\n",
    "\n",
    "shuffle(meta_df)\n",
    "\n",
    "train_files = meta_df[meta_df[\"split\"] == \"train\"][\"audio_filename\"].values\n",
    "val_files = meta_df[meta_df[\"split\"] == \"validation\"][\"audio_filename\"].values\n",
    "test_files = meta_df[meta_df[\"split\"] == \"test\"][\"audio_filename\"].values\n",
    "\n",
    "print(train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader init\n",
    "\n",
    "from util import *\n",
    "\n",
    "chunk_size = 20\n",
    "window_size = 2048\n",
    "window_overlap = 1024\n",
    "batch_size = 8\n",
    "\n",
    "train_val_data_file = \"data/maestro2017_train_val_pv.pkl\"\n",
    "# test_files = ['data/05_8K.wav']#, 'data/02_8K.wav', 'data/03_8K.wav', 'data/04_8K.wav', 'data/05_8K.wav']\n",
    "# test_files = in_files\n",
    "\n",
    "if os.path.exists(train_val_data_file): \n",
    "    train_gen, val_gen = pickle.load(open(train_val_data_file, 'rb'))\n",
    "else:\n",
    "    train_gen = DataGenerator(train_files, chunk_size, window_size, window_overlap, batch_size, vocoder=True)\n",
    "    val_gen = DataGenerator(val_files, chunk_size, window_size, window_overlap, batch_size, vocoder=True)\n",
    "    pickle.dump((train_gen, val_gen), open(train_val_data_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "\n",
    "from lstm import *\n",
    "\n",
    "input_dim = train_gen.X_list[0].shape[2] #TODO\n",
    "hidden_dim = 4096\n",
    "num_layers = 1\n",
    "print(input_dim)\n",
    "\n",
    "# model = LSTMBasic(input_dim, hidden_dim, num_layers=num_layers, batch_size=batch_size)\n",
    "# model = LSTMFC(input_dim, hidden_dim, hidden_dim, num_layers=num_layers, batch_size=batch_size, dropout_p=0.2)\n",
    "model = LSTMCNN(input_dim, hidden_dim, num_layers=num_layers, batch_size=batch_size, decoder=\"2fc\", dropout_p=0.5)\n",
    "\n",
    "model = model.to(computing_device)\n",
    "criterion = nn.MSELoss().to(computing_device)\n",
    "dp = nn.DataParallel(model, dim=1, device_ids=[2,3]).to(computing_device)\n",
    "m = dp.module\n",
    "optimizer = torch.optim.Adam(dp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# trainer setup\n",
    "\n",
    "from lstm_trainer import *\n",
    "trainer = LSTMTrainer(dp, criterion, optimizer, session_name=\"pv_dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model? \n",
    "\n",
    "load_model = False\n",
    "\n",
    "if load_model:\n",
    "    epochs_trained = 1000\n",
    "    model_file = \"models/pv_dropout/cs{}_h{}_e{}.ckpt\".format(chunk_size, hidden_dim, epochs_trained)\n",
    "#     model_file = \"models/v_cs20_h2048_e4000.ckpt\"\n",
    "    print(\"Loading model: {}\".format(model_file))\n",
    "    trainer.load_model(model_file, epochs_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 0039 | MIDI-Unprocessed_053 | Chunk 027 [########################------]078.8% | T+5:30:18.737808\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training\n",
    "\n",
    "train_model = True\n",
    "iter_epochs = 100\n",
    "iters = 50\n",
    "dump_epochs = 10\n",
    "\n",
    "if train_model:\n",
    "    \n",
    "#     fig = plt.figure(figsize=(6,3))\n",
    "#     ax = fig.add_subplot(1,1,1)\n",
    "#     fig.show(); fig.canvas.draw()\n",
    "    \n",
    "    # train a series of models at different numbers of epochs\n",
    "    curr_train_losses, curr_val_losses = [], []\n",
    "    for i in range(iters):\n",
    "\n",
    "        train_loss, val_loss = trainer.train(train_gen, val_gen, iter_epochs, 1,\n",
    "                                             dump_model=True, dump_epochs=dump_epochs, dump_loss=True)\n",
    "        curr_train_losses += train_loss  # train_loss is a 2D python list\n",
    "        curr_val_losses += val_loss\n",
    "        \n",
    "        # plot loss curve\n",
    "#         ax.clear()\n",
    "#         ax.plot(np.array(curr_train_losses).mean(axis=1))\n",
    "#         fig.canvas.draw()\n",
    "        \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "\n",
    "# train_loss = []\n",
    "# val_loss = []\n",
    "# for i in range(80):\n",
    "#     t,v = pickle.load(open(\"models/model_h150_e{}.ckpt.loss.pkl\".format((i+1)*10), 'rb'))\n",
    "#     train_loss += [t]\n",
    "#     val_loss += [v]\n",
    "# plt.plot(np.average(np.array(train_loss).reshape((800,3799)), axis=1))\n",
    "# plt.plot(np.average(np.array(val_loss).reshape((800,674)), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misc. tests below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"STOP\") # dirty way to stop the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 20\n",
    "window_size = 2048\n",
    "#window_overlap = 1023\n",
    "batch_size = 1\n",
    "\n",
    "train_files = ['feather.wav'] \n",
    "#, 'data/02_8K.wav', 'data/03_8K.wav', 'data/04_8K.wav', 'data/05_8K.wav']\n",
    "\n",
    "test_gen = DataGenerator(train_files, chunk_size, window_size, window_overlap, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, X, T = test_gen[0]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    m.curr_state = m.init_hidden()\n",
    "\n",
    "    eval_output = []\n",
    "    cell_states = []\n",
    "    hidden_states = []\n",
    "    \n",
    "    # prime the model with 30 seconds of input\n",
    "    primer = X[:600].to(computing_device)\n",
    "    for i in range(600):\n",
    "        out, states = m(primer[i:i+1], m.curr_state)\n",
    "        eval_output += [out.cpu().numpy()]\n",
    "        cell_states += [states[0].cpu().numpy()]\n",
    "        hidden_states += [states[1].cpu().numpy()]\n",
    "\n",
    "    # start generation \n",
    "    for i in range(200):\n",
    "        print(\"{}/{}\".format(i+1, 200), end='\\r')\n",
    "        out, states = m(out, m.curr_state)\n",
    "        eval_output += [out.cpu().numpy()]\n",
    "        cell_states += [states[0].cpu().numpy()]\n",
    "        hidden_states += [states[1].cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eo = torch.FloatTensor(eval_output)[:, 0]\n",
    "eo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname, X, T = train_gen[0]\n",
    "#print(X.shape)\n",
    "#print(X[0:chunk_size].shape)\n",
    "#chunks = torch.tensor(e[i*chunk_size*10:(i+1)*chunk_size*10])\n",
    "#print(chunks.shape)\n",
    "print(eo.min(), eo.max())\n",
    "t,x = train_gen.reassemble_istft(eo[400:, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fs = 8000\n",
    "\n",
    "plt.specgram(x[:1000], Fs=fs, NFFT=1024, noverlap=window_overlap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.load('biiig_test.npy')\n",
    "#t = np.load('feather_stft.npy')\n",
    "\n",
    "print(t.shape)\n",
    "frames = np.fft.irfft(t).real\n",
    "print(frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavy = []\n",
    "for x in frames:\n",
    "    # apply hanning window to frame\n",
    "    print(x.shape)\n",
    "    time = np.arange(window_size)\n",
    "    hanning = 0.5 * (1 - np.cos(2 * np.pi * time / window_size))\n",
    "    x *= hanning\n",
    "    \n",
    "    \n",
    "    #np.clip(buffer, -1, 1, out=buffer)\n",
    "\n",
    "    #n = buffer.shape[1]\n",
    "    #frames = (buffer.T.reshape((-1,)) * 32676).astype(np.int16).tobytes()\n",
    "    #self._writer.writeframes(frames) lmao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfile.write(\"test2.wav\", fs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_states = np.array(cell_states)[:, 0, 0]\n",
    "hidden_states = np.array(hidden_states)[:, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(eo[:,0].transpose(0,1), cmap='gray')\n",
    "plt.show()\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(cell_states.transpose(), cmap='gray')\n",
    "plt.show()\n",
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
